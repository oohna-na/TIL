## Ensemble History

### 1. Decision Tree

- 개념

    - 데이터를 조건에 따라 반복적으로 나누어 예측값을 생성하는 단일 모델델

    - 트리가 깊어질수록 특정 데이터에 과적합(overfitting)되는 경향이 있음

- 한계

    - 단일 트리 모델은 예측 성능이 제학적이고 과적합 문제를 해결하기 어려움


### 2. Bagging

- 개념

    - 데이터의 샘플링을 통해 여러 트리를 학습시키고, 이들의 결과를 결합해 예측 성능을 향상시키는 방법

- 작동 원리

    1. 데이터에서 복원 추출(bootstrap)로 여러 샘플 생성

    2. 각 샘플로 독립적인 결정 트리 학습

    3. 결과를 다수결(voting, 분류) 또는 평균(회귀)으로 결합

- 장점

    - 모델 간 상관관계를 줄여 과적합 문제 완화

    - 더 안정적이고 신뢰할 수 있는 예측 제공

- 대표 모델: Random Forest


### 3. Boosting

- 개념

    - 순차적으로 모델을 학습하며 이전 모델이 잘못 예측한 샘플에 가중치를 두어 성능을 개선하는 방법

    - 여러 Weak Learner를 결합해 강력한 모델을 만듦

- 작동 원리

    1. 첫 번째 모델 학습 후 오차 계산

    2. 다음 모델은 이전 모델의 오차를 줄이는 방향으로 학습

    3. 최종적으로 모든 모델의 결과를 결합

- 대표 모델: Gradient Boosting


### 4. Gradient Boosting

- 개념

    - Boosting 방법론을 기반으로 잔여 오차(residual error)를 예측하도록 설계된 모델

    - 각 단계에서 이전 모델의 오차를 줄이기 위해 Gradient Descent(경사 하강법)를 활용

- 장점

    - 분류와 회귀 문제에서 높은 성능

    - 과적합을 제어할 수 있는 다양한 하이퍼파라미터 제공

- 한계

    - 계산 비용이 높고 학습 속도가 느림

### 5. XGBoost

- 개념

    - Gradient Boosting의 성능을 개선한 모델로 효율성과 정확도를 극대화

    - 병렬 처리와 L1/L2 정규화를 활용해 과적합 방지

- 특징

    - Regularization(정규화): 모델 복잡도를 제어하여 과적합 방지

    - 병렬 학습: 학습 속도를 크게 향상

    - 스케일링: 대규모 데이터셋에서도 빠르게 처리 가능


### 6. LightGBM

- 개념

    - Gradient Boosting을 기반으로 더 가볍고 빠르게 학습하도록 최적화

    - 리프-와이즈(Tree Leaf-wise) 방식으로 트리를 확장

- 특징

    - 속도: XGBoost보다 학습 속도가 빠름

    - 메모리 효율성: 메모리 사용량이 적음

    - 대규모 데이터셋: 범주형 데이터와 대규모 데이터 처리에 강점


### 7. CatBoost

- 개념

    - Gradient Boosting의 발전된 형태로, 범주형 데이터 처리를 자동화하여 성능을 향상

    - 데이터 전처리 없이도 범주형 데이터를 학습할 수 있음

- 특징

    - 자동 범주형 데이터 처리: One-hot encoding 같은 별도의 변환 없이 처리

    - 빠른 학습 속도: LightGBM처럼 학습 속도가 빠름

    - 과적합 방지: 정교한 정규화를 적용
    

### 발전 흐름

#### 1단계: 단일 모델
- Decision Trees: 단일 트리를 활용한 예측

#### 2단계: Bagging
- Random Forest: 여러 트리를 병렬적으로 결합해 안전성과 예측력을 개선

#### 3단계: Boosting
- Gradient Boosting: 순차적 학습으로 오차를 줄임

- XGBoost, LightGBM, CatBoost: 부스팅을 효율화 및 고도화한 모델