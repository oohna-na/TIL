## Feature Selection 변수 선택

- 학습에 필요한 변수(특성)의 중요도에 따라 변수를 선택하는 과정

- 모델 학습에 불필요한 변수를 제거하여 효율성과 성능을 향상

- 핵심은 변수의 중요도를 어떻게 정의하고 평가할지에 대한 방법론을 사용하는 것

### 변수 선택을 수행하는 이유

1. 차원의 저주(Curse of Dimensionality) 해소

    - 데이터의 차원이 증가하면 학습 데이터의 희소성이 커지고, 모델의 복잡도가 증가하여 성능이 저하되는 현상

    - 불필요한 변수들을 제거함으로써 차원을 줄여 모델의 복잡도를 낮추고 성능을 향상시킴

2. 과적합(Overfitting) 완화

    - 너무 많은 변수를 포함하면 모델이 학습 데이터에 과도하게 적응하여 일반화 성능이 떨어질 수 있음

    - 중요한 변수만 선택하여 과적합 문제를 완화

3. 학습 및 추론 시간, 메모리 개선

    - 변수가 적어지면 학습 속도가 빨라지고 메모리 사용량이 줄어들어 자원 효율성이 높아짐

4. 해석 가능성 증가

    - 더 적은 변수만 포함하면 결과를 해석하거나 설명하기가 용이해짐

    - 비즈니스와 연구 관점에서 이해하기 쉬운 모델을 제공



### 대표적인 변수 선택의 3가지 접근법

#### 1. Filter Methods

- 변수들 간의 통계적 관계를 기반으로 변수의 중요도를 평가하고 선택하는 방법

- 모델 학습과는 독립적으로 동작하며, 데이터의 분포나 상관관계를 기반으로 단순히 변수를 선택

- 예시

    - 상관관계 분석: 변수 간의 상관계수를 계산하여 중요한 변수를 선택

    - 분산 기반 방법: 분산이 낮은 변수를 제거 (ex. Variance Threshold)

- 장점

    - 계산 속도가 빠르고, 직관적이며 간단

    - 모델 학습에 앞서 전처리 단계에서 적용 가능

- 단점

    - 모델과의 상호작용을 고려하지 않기 때문에 성능이 떨어질 수 있음


#### 2. Wrapper Methods

- 실제 머신러닝 모델의 성능을 기준으로 변수의 중요도를 평가하고 선택하는 방법

- 모델 학습을 반복적으로 수행하면서 변수를 추가하거나 제거해 최적의 변수 조합을 찾음

- 예시

    -  Forward Selection: 변수를 하나씩 추가하면서 성능이 개선되는지를 평가

    - Backward Elimination: 모든 변수를 시작으로 하나씩 제거하며 성능을 평가

- 장점

    - 모델 성능에 직접적으로 기여하는 변수를 선택할 수 있음

- 단점

    - 계산 비용이 높고, 데이터셋이 클 경우 시간이 많이 소요됨

#### 3. Embedded Methods

- 변수 선택이 모델 학습 과정에 직접 포함되는 방법

- 모델 자체가 변수 중요도를 평가하거나 정규화 기반 기법을 통해 선택 과정을 수행

- 예시

    - Feature Importance: 트리 기반 모델(ex. Random Forest, XGBoost)이 제공하는 변수 중요도 점수

    - Regularizer 기반 방법: Lasso(라쏘)와 같은 정규화를 사용하여 중요하지 않은 변수의 가중치를 0으로 설정

- 장점

    - 성능과 효율성의 균형을 맞춤

    - 계산 비용이 비교적 적고, 모델과의 상호작용을 반영

- 단점

    - 특정 모델에 의존적일 수 있음


